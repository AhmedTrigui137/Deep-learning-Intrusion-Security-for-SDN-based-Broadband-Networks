{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scapy scikit-learn torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzdTw5De2w36",
        "outputId": "cb6f03ed-d00f-41de-c48c-721fd3827d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting scapy\n",
            "  Downloading scapy-2.6.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading scapy-2.6.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scapy\n",
            "Successfully installed scapy-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score\n",
        "\n",
        "# Load Data and Create Synthetic Labels\n",
        "def load_and_label_data(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Handle potential infinite and NaN values initially\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        # Encode 'Label' column if it exists, otherwise create synthetic labels\n",
        "        if 'Label' in df.columns:\n",
        "            le = LabelEncoder()\n",
        "            df['Label'] = le.fit_transform(df['Label'])\n",
        "        else:\n",
        "            df['Synthetic_Label'] = (df['Total Length of Fwd Packets'] > 500).astype(int)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error reading file:\", e)\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df):\n",
        "\n",
        "    drop_columns = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp']\n",
        "    df = df.drop(columns=[col for col in drop_columns if col in df.columns], errors='ignore')\n",
        "\n",
        "\n",
        "    label_col = 'Label' if 'Label' in df.columns else 'Synthetic_Label'\n",
        "    non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    df = df.drop(columns=non_numeric_cols, errors='ignore')\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.fillna(df.mean(), inplace=True)  # Example: Fill NaN with column means\n",
        "\n",
        "    X = df.drop(columns=[label_col]).values\n",
        "    y = df[label_col].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Define Simple Neural Network Model\n",
        "class IntrusionDetector(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(IntrusionDetector, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def main():\n",
        "    # Load and label the dataset\n",
        "    file_path = '/root/.cache/kagglehub/datasets/chethuhn/network-intrusion-dataset/versions/1/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'  # Path to your dataset\n",
        "    df = load_and_label_data(file_path)\n",
        "    if df is not None:\n",
        "        X, y = preprocess_data(df)\n",
        "\n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train))\n",
        "        test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test))\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "        test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Initialize model, criterion, and optimizer\n",
        "        input_size = X.shape[1]\n",
        "        num_classes = len(np.unique(y))\n",
        "        model = IntrusionDetector(input_size, num_classes)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Train the model\n",
        "        model.train()\n",
        "        num_epochs = 20\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "        # Evaluate the model\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                all_preds.extend(predicted.numpy())\n",
        "                all_labels.extend(labels.numpy())\n",
        "\n",
        "        # Calculate and output accuracy and precision\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision = precision_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "        print(f'Accuracy: {accuracy:.4f}')\n",
        "        print(f'Precision: {precision:.4f}')\n",
        "\n",
        "        # Classification report\n",
        "        print(classification_report(all_labels, all_preds))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-jSDXZuasFs",
        "outputId": "ede969b8-c9e8-4835-823a-af4d1207d2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.0140\n",
            "Epoch [2/20], Loss: 0.0040\n",
            "Epoch [3/20], Loss: 0.0032\n",
            "Epoch [4/20], Loss: 0.0029\n",
            "Epoch [5/20], Loss: 0.0025\n",
            "Epoch [6/20], Loss: 0.0022\n",
            "Epoch [7/20], Loss: 0.0022\n",
            "Epoch [8/20], Loss: 0.0023\n",
            "Epoch [9/20], Loss: 0.0020\n",
            "Epoch [10/20], Loss: 0.0019\n",
            "Epoch [11/20], Loss: 0.0016\n",
            "Epoch [12/20], Loss: 0.0018\n",
            "Epoch [13/20], Loss: 0.0017\n",
            "Epoch [14/20], Loss: 0.0018\n",
            "Epoch [15/20], Loss: 0.0017\n",
            "Epoch [16/20], Loss: 0.0014\n",
            "Epoch [17/20], Loss: 0.0014\n",
            "Epoch [18/20], Loss: 0.0017\n",
            "Epoch [19/20], Loss: 0.0015\n",
            "Epoch [20/20], Loss: 0.0014\n",
            "Accuracy: 0.9993\n",
            "Precision: 0.9983\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     39251\n",
            "           1       1.00      1.00      1.00      5898\n",
            "\n",
            "    accuracy                           1.00     45149\n",
            "   macro avg       1.00      1.00      1.00     45149\n",
            "weighted avg       1.00      1.00      1.00     45149\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score\n",
        "\n",
        "# Load Data and Create Synthetic Labels\n",
        "def load_and_label_data(file_path, num_rows=19000):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, nrows=num_rows)\n",
        "\n",
        "        if 'Label' in df.columns:\n",
        "            le = LabelEncoder()\n",
        "            df['Label'] = le.fit_transform(df['Label'].fillna('Unknown'))\n",
        "\n",
        "        if 'Label' not in df:\n",
        "            df['Synthetic_Label'] = (df['Total Length of Fwd Packets'] > 500).astype(int)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error reading file:\", e)\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df):\n",
        "    drop_columns = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Timestamp']\n",
        "    df = df.drop(columns=[col for col in drop_columns if col in df.columns], errors='ignore')\n",
        "\n",
        "    label_col = 'Label' if 'Label' in df.columns else 'Synthetic_Label'\n",
        "\n",
        "    non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    df = df.drop(columns=non_numeric_cols, errors='ignore')\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "    X = df.drop(columns=[label_col]).values\n",
        "    y = df[label_col].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Define Simple Neural Network Model\n",
        "class IntrusionDetector(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(IntrusionDetector, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def evaluate_model(X, y, num_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    results = []\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train))\n",
        "        test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test))\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "        test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "        model = IntrusionDetector(input_size=X.shape[1], num_classes=len(np.unique(y)))\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(10):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch [{epoch+1}/10], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                all_preds.extend(predicted.numpy())\n",
        "                all_labels.extend(labels.numpy())\n",
        "\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision = precision_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "        print(f'Fold Accuracy: {accuracy:.4f}')\n",
        "        print(f'Fold Precision: {precision:.4f}')\n",
        "        results.append((accuracy, precision))\n",
        "\n",
        "    mean_accuracy, mean_precision = np.mean(results, axis=0)\n",
        "    print(f'Mean Accuracy: {mean_accuracy:.4f}, Mean Precision: {mean_precision:.4f}')\n",
        "\n",
        "def main():\n",
        "    file_path = '/root/.cache/kagglehub/datasets/chethuhn/network-intrusion-dataset/versions/1/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
        "    df = load_and_label_data(file_path, num_rows=1000)\n",
        "\n",
        "    if df is not None:\n",
        "        X, y = preprocess_data(df)\n",
        "        evaluate_model(X, y)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoCVppSacWMh",
        "outputId": "d31515dc-af67-48c0-95d3-dac81d114759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5727\n",
            "Epoch [2/10], Loss: 0.2615\n",
            "Epoch [3/10], Loss: 0.0861\n",
            "Epoch [4/10], Loss: 0.0469\n",
            "Epoch [5/10], Loss: 0.0341\n",
            "Epoch [6/10], Loss: 0.0326\n",
            "Epoch [7/10], Loss: 0.0255\n",
            "Epoch [8/10], Loss: 0.0229\n",
            "Epoch [9/10], Loss: 0.0215\n",
            "Epoch [10/10], Loss: 0.0187\n",
            "Fold Accuracy: 0.9950\n",
            "Fold Precision: 0.9926\n",
            "Epoch [1/10], Loss: 0.5695\n",
            "Epoch [2/10], Loss: 0.2363\n",
            "Epoch [3/10], Loss: 0.0791\n",
            "Epoch [4/10], Loss: 0.0449\n",
            "Epoch [5/10], Loss: 0.0371\n",
            "Epoch [6/10], Loss: 0.0300\n",
            "Epoch [7/10], Loss: 0.0270\n",
            "Epoch [8/10], Loss: 0.0242\n",
            "Epoch [9/10], Loss: 0.0223\n",
            "Epoch [10/10], Loss: 0.0217\n",
            "Fold Accuracy: 0.9950\n",
            "Fold Precision: 0.9963\n",
            "Epoch [1/10], Loss: 0.5575\n",
            "Epoch [2/10], Loss: 0.2407\n",
            "Epoch [3/10], Loss: 0.0845\n",
            "Epoch [4/10], Loss: 0.0462\n",
            "Epoch [5/10], Loss: 0.0378\n",
            "Epoch [6/10], Loss: 0.0321\n",
            "Epoch [7/10], Loss: 0.0299\n",
            "Epoch [8/10], Loss: 0.0255\n",
            "Epoch [9/10], Loss: 0.0250\n",
            "Epoch [10/10], Loss: 0.0207\n",
            "Fold Accuracy: 0.9900\n",
            "Fold Precision: 0.9857\n",
            "Epoch [1/10], Loss: 0.5480\n",
            "Epoch [2/10], Loss: 0.2033\n",
            "Epoch [3/10], Loss: 0.0623\n",
            "Epoch [4/10], Loss: 0.0359\n",
            "Epoch [5/10], Loss: 0.0265\n",
            "Epoch [6/10], Loss: 0.0219\n",
            "Epoch [7/10], Loss: 0.0200\n",
            "Epoch [8/10], Loss: 0.0179\n",
            "Epoch [9/10], Loss: 0.0174\n",
            "Epoch [10/10], Loss: 0.0147\n",
            "Fold Accuracy: 0.9700\n",
            "Fold Precision: 0.9783\n",
            "Epoch [1/10], Loss: 0.4893\n",
            "Epoch [2/10], Loss: 0.1660\n",
            "Epoch [3/10], Loss: 0.0584\n",
            "Epoch [4/10], Loss: 0.0349\n",
            "Epoch [5/10], Loss: 0.0293\n",
            "Epoch [6/10], Loss: 0.0251\n",
            "Epoch [7/10], Loss: 0.0236\n",
            "Epoch [8/10], Loss: 0.0206\n",
            "Epoch [9/10], Loss: 0.0189\n",
            "Epoch [10/10], Loss: 0.0181\n",
            "Fold Accuracy: 0.9850\n",
            "Fold Precision: 0.9850\n",
            "Mean Accuracy: 0.9870, Mean Precision: 0.9876\n"
          ]
        }
      ]
    }
  ]
}